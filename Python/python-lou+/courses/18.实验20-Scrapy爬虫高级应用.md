
#### 知识点
- 页面追随
- 图片下载
- Item 包含多个页面数据
- 模拟登录


## 页面追随

在前面实现课程爬虫和用户爬虫中，因为实验楼的课程和用户 url 都是通过 id 来构造的，所以可以轻松构造一批顺序的 urls 给爬虫。但是在很多网站中，url 并不是那么轻松构造的，更常用的方法是从一个或者多个链接（start_urls）爬取页面后，再从页面中解析需要的链接继续爬取，不断循环。
下面是一个简单的例子，在实验楼课程编号为 63 的课程主页，从课程相关的进阶课程获取下一批要爬取的课程。用前面所学的知识就能够完成这个程序，在看下面的代码前可以思考下怎么实现。

结合前面所学的知识，你可能会写出类似这样的代码：
```
# -*- coding: utf-8 -*-
import scrapy


class CoursesFollowSpider(scrapy.Spider):
    name = 'courses_follow'
    start_urls = ['https://shiyanlou.com/courses/63']

    def parse(self, response):
        yield {
            'name': response.xpath('//h4[@class="course-infobox-title"]/span/text()').extract_first(),
            'author': response.xpath('//div[@class="mooc-info"]/div[@class="name"]/strong/text()').extract_first()
        }
        # 从返回的 response 解析出“进阶课程”里的课程链接，依次构造
        # 请求，再将本函数指定为回调函数，类似递归
        for url in response.xpath('//div[@class="sidebox-body course-content"]/a/@href').extract():
            # 解析出的 url 是相对 url，可以手动将它构造为全 url
            # 或者使用 response.urljoin() 函数
            yield scrapy.Request(url=response.urljoin(url), callback=self.parse)
```
完成页面跟随的核心就是最后 for 循环的代码。使用response.follow 函数可以对 for 循环代码做进一步简化：
```
# -*- coding: utf-8 -*-
import scrapy


class CoursesFollowSpider(scrapy.Spider):
    name = 'courses_follow'
    start_urls = ['https://shiyanlou.com/courses/63']

    def parse(self, response):
        yield {
            'name': response.xpath('//h4[@class="course-infobox-title"]/span/text()').extract_first(),
            'author': response.xpath('//div[@class="mooc-info"]/div[@class="name"]/strong/text()').extract_first()
        }
        # 不需要 extract 了
        for url in response.xpath('//div[@class="sidebox-body course-content"]/a/@href'):
            # 不需要构造全 url 了
            yield response.follow(url, callback=self.parse)

```

## 图片下载

            
